{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pretty_midi\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.layers import BatchNormalization as BatchNorm\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Lambda\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NoteTokenizer import NoteTokenizer\n",
    "from inputs_preprocess_utils import midi_to_piano_rolls, piano_rolls_to_times_notes_dict, encode_notes_dict_with_duration, add_empty_note_to_dict, generate_input_and_target "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess dataset & Prepare network input and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameter\n",
    "sequence_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the input data\n",
    "pieces_rolls_dict = midi_to_piano_rolls(\"dataset/Chopin/filtered/*.mid\")\n",
    "times_notes_dict_list = piano_rolls_to_times_notes_dict(pieces_rolls_dict)\n",
    "times_notes_dict_list = add_empty_note_to_dict(times_notes_dict_list)\n",
    "times_notes_dict_list = encode_notes_dict_with_duration(times_notes_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  445\n"
     ]
    }
   ],
   "source": [
    "# Initialize the first note_tokenizer object\n",
    "note_tokenizer = NoteTokenizer()\n",
    "\n",
    "# Populate the NoteTokenizer class withh our tuples\n",
    "for piece in times_notes_dict_list:\n",
    "    note_tokenizer.partial_fit(list(piece.values()))\n",
    "    \n",
    "# note_tokenizer.add_new_note('e')\n",
    "\n",
    "if ('e', 1) not in note_tokenizer.notes_to_index:\n",
    "    print(\"add the empty class\")\n",
    "    note_tokenizer.add_new_note(('e', 1))\n",
    "    \n",
    "unique_notes = note_tokenizer.unique_word\n",
    "print(\"Number of classes: \", unique_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the network input and output in notes form\n",
    "# input: a sequence of notes of length \"sequence_length\"\n",
    "# output: the next notes after each sequence\n",
    "collected_list_input, collected_list_target = [], []\n",
    "for times_notes_dict in times_notes_dict_list:\n",
    "    list_training, list_target = generate_input_and_target(times_notes_dict, sequence_length)\n",
    "    collected_list_input += list_training\n",
    "    collected_list_target += list_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the network input and output: transform from str to index representation\n",
    "for index, curr_list in enumerate(collected_list_input):\n",
    "    collected_list_input[index] = note_tokenizer.transform(curr_list)\n",
    "    \n",
    "for index, curr_notes in enumerate(collected_list_target):\n",
    "    collected_list_target[index] = note_tokenizer.transform(curr_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final preparation...\n",
    "# convert them to numpy array\n",
    "collected_list_input = np.array(collected_list_input)\n",
    "collected_list_target = np.array(collected_list_target)\n",
    "\n",
    "# reshape the input into a format compatible with LSTM layers\n",
    "network_input = np.reshape(collected_list_input, (collected_list_input.shape[0], collected_list_input.shape[1], 1))\n",
    "\n",
    "# need to adjust the number of unique_notes!!! (don't know why tho...) need to figure out!!!\n",
    "n_vocab = unique_notes - 5\n",
    "\n",
    "# normalize input: \n",
    "network_input = network_input / float(n_vocab)\n",
    "\n",
    "# one-hot encoding the network output\n",
    "network_output = np_utils.to_categorical(collected_list_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3312, 10, 1)\n",
      "(3312, 440)\n"
     ]
    }
   ],
   "source": [
    "print(network_input.shape)\n",
    "print(network_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(network_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Network & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(network_input, n_vocab):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(\n",
    "        512,\n",
    "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "        recurrent_dropout=0.3,\n",
    "        return_sequences=True\n",
    "    ))\n",
    "    \n",
    "    ''' Your Code Here '''\n",
    "    model.add(LSTM(\n",
    "        512,\n",
    "        recurrent_dropout=0.3,\n",
    "        return_sequences=False    # don't return a sequence here\n",
    "    ))\n",
    "\n",
    "    model.add(BatchNorm())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256,activation=\"relu\"))\n",
    "    model.add(BatchNorm())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Lambda(lambda x: x / 0.6))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(optimizer = Adam(),\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 10, 512)           1052672   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 512)               2099200   \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 440)               113080    \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 440)               0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 440)               0         \n",
      "=================================================================\n",
      "Total params: 3,399,352\n",
      "Trainable params: 3,397,816\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training...\n",
      "Epoch 1/200\n",
      "52/52 [==============================] - 50s 410ms/step - loss: 6.8230 - accuracy: 0.0133\n",
      "Epoch 2/200\n",
      "52/52 [==============================] - 21s 396ms/step - loss: 5.8105 - accuracy: 0.0592\n",
      "Epoch 3/200\n",
      "52/52 [==============================] - 20s 376ms/step - loss: 5.5720 - accuracy: 0.0608\n",
      "Epoch 4/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 5.2956 - accuracy: 0.0888\n",
      "Epoch 5/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 4.9726 - accuracy: 0.1301\n",
      "Epoch 6/200\n",
      "52/52 [==============================] - 20s 386ms/step - loss: 4.8220 - accuracy: 0.1356\n",
      "Epoch 7/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 4.6456 - accuracy: 0.1312\n",
      "Epoch 8/200\n",
      "52/52 [==============================] - 20s 394ms/step - loss: 4.4662 - accuracy: 0.1413\n",
      "Epoch 9/200\n",
      "52/52 [==============================] - 20s 385ms/step - loss: 4.3235 - accuracy: 0.1309\n",
      "Epoch 10/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 4.0678 - accuracy: 0.1539\n",
      "Epoch 11/200\n",
      "52/52 [==============================] - 20s 394ms/step - loss: 3.9165 - accuracy: 0.1642\n",
      "Epoch 12/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 3.7470 - accuracy: 0.1634\n",
      "Epoch 13/200\n",
      "52/52 [==============================] - 21s 396ms/step - loss: 3.5970 - accuracy: 0.1740\n",
      "Epoch 14/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 3.4780 - accuracy: 0.1907\n",
      "Epoch 15/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 3.3049 - accuracy: 0.2096\n",
      "Epoch 16/200\n",
      "52/52 [==============================] - 20s 384ms/step - loss: 3.1403 - accuracy: 0.2255\n",
      "Epoch 17/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 2.9951 - accuracy: 0.2611\n",
      "Epoch 18/200\n",
      "52/52 [==============================] - 20s 386ms/step - loss: 2.8352 - accuracy: 0.2801\n",
      "Epoch 19/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 2.7088 - accuracy: 0.3050\n",
      "Epoch 20/200\n",
      "52/52 [==============================] - 20s 384ms/step - loss: 2.5737 - accuracy: 0.3189\n",
      "Epoch 21/200\n",
      "52/52 [==============================] - 20s 377ms/step - loss: 2.4078 - accuracy: 0.3622\n",
      "Epoch 22/200\n",
      "52/52 [==============================] - 20s 382ms/step - loss: 2.2896 - accuracy: 0.3784\n",
      "Epoch 23/200\n",
      "52/52 [==============================] - 20s 394ms/step - loss: 2.0926 - accuracy: 0.4309\n",
      "Epoch 24/200\n",
      "52/52 [==============================] - 20s 394ms/step - loss: 1.9763 - accuracy: 0.4581\n",
      "Epoch 25/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 1.8600 - accuracy: 0.4812\n",
      "Epoch 26/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 1.7559 - accuracy: 0.5063\n",
      "Epoch 27/200\n",
      "52/52 [==============================] - 20s 386ms/step - loss: 1.6314 - accuracy: 0.5409\n",
      "Epoch 28/200\n",
      "52/52 [==============================] - 20s 380ms/step - loss: 1.5482 - accuracy: 0.5668\n",
      "Epoch 29/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 1.4043 - accuracy: 0.5851\n",
      "Epoch 30/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 1.3088 - accuracy: 0.6217\n",
      "Epoch 31/200\n",
      "52/52 [==============================] - 20s 383ms/step - loss: 1.2462 - accuracy: 0.6430\n",
      "Epoch 32/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 1.2094 - accuracy: 0.6560\n",
      "Epoch 33/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 1.0454 - accuracy: 0.6908\n",
      "Epoch 34/200\n",
      "52/52 [==============================] - 20s 383ms/step - loss: 1.0029 - accuracy: 0.6978\n",
      "Epoch 35/200\n",
      "52/52 [==============================] - 21s 394ms/step - loss: 0.9203 - accuracy: 0.7204\n",
      "Epoch 36/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.8185 - accuracy: 0.7578\n",
      "Epoch 37/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.8047 - accuracy: 0.7605\n",
      "Epoch 38/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.7789 - accuracy: 0.7656\n",
      "Epoch 39/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 0.7263 - accuracy: 0.7934\n",
      "Epoch 40/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.7728 - accuracy: 0.7655\n",
      "Epoch 41/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.6646 - accuracy: 0.7963\n",
      "Epoch 42/200\n",
      "52/52 [==============================] - 20s 386ms/step - loss: 0.6530 - accuracy: 0.8028\n",
      "Epoch 43/200\n",
      "52/52 [==============================] - 20s 386ms/step - loss: 0.6330 - accuracy: 0.8108\n",
      "Epoch 44/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.6098 - accuracy: 0.8079\n",
      "Epoch 45/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.6177 - accuracy: 0.8184\n",
      "Epoch 46/200\n",
      "52/52 [==============================] - 20s 384ms/step - loss: 0.6122 - accuracy: 0.8133\n",
      "Epoch 47/200\n",
      "52/52 [==============================] - 20s 380ms/step - loss: 0.5725 - accuracy: 0.8255\n",
      "Epoch 48/200\n",
      "52/52 [==============================] - 20s 382ms/step - loss: 0.5488 - accuracy: 0.8377\n",
      "Epoch 49/200\n",
      "52/52 [==============================] - 20s 387ms/step - loss: 0.5308 - accuracy: 0.8364\n",
      "Epoch 50/200\n",
      "52/52 [==============================] - 20s 387ms/step - loss: 0.5393 - accuracy: 0.8327\n",
      "Epoch 51/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.4804 - accuracy: 0.8427\n",
      "Epoch 52/200\n",
      "52/52 [==============================] - 20s 389ms/step - loss: 0.4602 - accuracy: 0.8653\n",
      "Epoch 53/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 0.5075 - accuracy: 0.8364\n",
      "Epoch 54/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.4743 - accuracy: 0.8468\n",
      "Epoch 55/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.4811 - accuracy: 0.8421\n",
      "Epoch 56/200\n",
      "52/52 [==============================] - 20s 386ms/step - loss: 0.5013 - accuracy: 0.8410\n",
      "Epoch 57/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.4537 - accuracy: 0.8576\n",
      "Epoch 58/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.4290 - accuracy: 0.8616\n",
      "Epoch 59/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 0.4540 - accuracy: 0.8505\n",
      "Epoch 60/200\n",
      "52/52 [==============================] - 20s 394ms/step - loss: 0.4031 - accuracy: 0.8657\n",
      "Epoch 61/200\n",
      "52/52 [==============================] - 20s 394ms/step - loss: 0.4072 - accuracy: 0.8712\n",
      "Epoch 62/200\n",
      "52/52 [==============================] - 20s 382ms/step - loss: 0.4161 - accuracy: 0.8585\n",
      "Epoch 63/200\n",
      "52/52 [==============================] - 20s 391ms/step - loss: 0.4422 - accuracy: 0.8583\n",
      "Epoch 64/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 20s 383ms/step - loss: 0.4289 - accuracy: 0.8545\n",
      "Epoch 65/200\n",
      "52/52 [==============================] - 20s 380ms/step - loss: 0.4427 - accuracy: 0.8583\n",
      "Epoch 66/200\n",
      "52/52 [==============================] - 20s 378ms/step - loss: 0.3768 - accuracy: 0.8704\n",
      "Epoch 67/200\n",
      "52/52 [==============================] - 20s 380ms/step - loss: 0.3935 - accuracy: 0.8665\n",
      "Epoch 68/200\n",
      "52/52 [==============================] - 20s 386ms/step - loss: 0.4038 - accuracy: 0.8654\n",
      "Epoch 69/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 0.4084 - accuracy: 0.8674\n",
      "Epoch 70/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.3894 - accuracy: 0.8711\n",
      "Epoch 71/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.4165 - accuracy: 0.8689\n",
      "Epoch 72/200\n",
      "52/52 [==============================] - 20s 389ms/step - loss: 0.3637 - accuracy: 0.8735\n",
      "Epoch 73/200\n",
      "52/52 [==============================] - 21s 396ms/step - loss: 0.3496 - accuracy: 0.8771\n",
      "Epoch 74/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 0.3504 - accuracy: 0.8780\n",
      "Epoch 75/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 0.3546 - accuracy: 0.8801\n",
      "Epoch 76/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.3460 - accuracy: 0.8791\n",
      "Epoch 77/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 0.3564 - accuracy: 0.8775\n",
      "Epoch 78/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.3572 - accuracy: 0.8766\n",
      "Epoch 79/200\n",
      "52/52 [==============================] - 20s 394ms/step - loss: 0.3429 - accuracy: 0.8767\n",
      "Epoch 80/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.3925 - accuracy: 0.8672\n",
      "Epoch 81/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.3405 - accuracy: 0.8856\n",
      "Epoch 82/200\n",
      "52/52 [==============================] - 20s 384ms/step - loss: 0.3586 - accuracy: 0.8772\n",
      "Epoch 83/200\n",
      "52/52 [==============================] - 20s 386ms/step - loss: 0.3401 - accuracy: 0.8888\n",
      "Epoch 84/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 0.3432 - accuracy: 0.8779\n",
      "Epoch 85/200\n",
      "52/52 [==============================] - 20s 386ms/step - loss: 0.3306 - accuracy: 0.8864\n",
      "Epoch 86/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.3263 - accuracy: 0.8878\n",
      "Epoch 87/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.3630 - accuracy: 0.8723\n",
      "Epoch 88/200\n",
      "52/52 [==============================] - 20s 387ms/step - loss: 0.3562 - accuracy: 0.8743\n",
      "Epoch 89/200\n",
      "52/52 [==============================] - 20s 384ms/step - loss: 0.3216 - accuracy: 0.8800\n",
      "Epoch 90/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.3504 - accuracy: 0.8816\n",
      "Epoch 91/200\n",
      "52/52 [==============================] - 20s 380ms/step - loss: 0.3308 - accuracy: 0.8847\n",
      "Epoch 92/200\n",
      "52/52 [==============================] - 20s 376ms/step - loss: 0.3085 - accuracy: 0.8956\n",
      "Epoch 93/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.2794 - accuracy: 0.8990\n",
      "Epoch 94/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 0.3126 - accuracy: 0.8952\n",
      "Epoch 95/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 0.3067 - accuracy: 0.8882\n",
      "Epoch 96/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.3434 - accuracy: 0.8817\n",
      "Epoch 97/200\n",
      "52/52 [==============================] - 20s 384ms/step - loss: 0.3042 - accuracy: 0.8862\n",
      "Epoch 98/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 0.2754 - accuracy: 0.8952\n",
      "Epoch 99/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.2777 - accuracy: 0.8941\n",
      "Epoch 100/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.3108 - accuracy: 0.8863\n",
      "Epoch 101/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.3020 - accuracy: 0.8966\n",
      "Epoch 102/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.2779 - accuracy: 0.8984\n",
      "Epoch 103/200\n",
      "52/52 [==============================] - 20s 387ms/step - loss: 0.3109 - accuracy: 0.8887\n",
      "Epoch 104/200\n",
      "52/52 [==============================] - 20s 394ms/step - loss: 0.2579 - accuracy: 0.9060\n",
      "Epoch 105/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.2954 - accuracy: 0.8945\n",
      "Epoch 106/200\n",
      "52/52 [==============================] - 20s 394ms/step - loss: 0.3170 - accuracy: 0.8816\n",
      "Epoch 107/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.2857 - accuracy: 0.8979\n",
      "Epoch 108/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.2784 - accuracy: 0.8941\n",
      "Epoch 109/200\n",
      "52/52 [==============================] - 20s 380ms/step - loss: 0.2912 - accuracy: 0.8899\n",
      "Epoch 110/200\n",
      "52/52 [==============================] - 20s 384ms/step - loss: 0.3706 - accuracy: 0.8751\n",
      "Epoch 111/200\n",
      "52/52 [==============================] - 20s 381ms/step - loss: 0.3018 - accuracy: 0.8904\n",
      "Epoch 112/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.2659 - accuracy: 0.8998\n",
      "Epoch 113/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 0.2904 - accuracy: 0.8923\n",
      "Epoch 114/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.2737 - accuracy: 0.8987\n",
      "Epoch 115/200\n",
      "52/52 [==============================] - 20s 382ms/step - loss: 0.2620 - accuracy: 0.9031\n",
      "Epoch 116/200\n",
      "52/52 [==============================] - 20s 384ms/step - loss: 0.2786 - accuracy: 0.8913\n",
      "Epoch 117/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.3027 - accuracy: 0.8855\n",
      "Epoch 118/200\n",
      "52/52 [==============================] - 20s 386ms/step - loss: 0.2723 - accuracy: 0.8965\n",
      "Epoch 119/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.2632 - accuracy: 0.8997\n",
      "Epoch 120/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.2605 - accuracy: 0.9049\n",
      "Epoch 121/200\n",
      "52/52 [==============================] - 20s 384ms/step - loss: 0.3042 - accuracy: 0.8934\n",
      "Epoch 122/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.3141 - accuracy: 0.8874\n",
      "Epoch 123/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 0.3118 - accuracy: 0.8910\n",
      "Epoch 124/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.2870 - accuracy: 0.8905\n",
      "Epoch 125/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.2922 - accuracy: 0.8961\n",
      "Epoch 126/200\n",
      "52/52 [==============================] - 20s 389ms/step - loss: 0.2652 - accuracy: 0.9001\n",
      "Epoch 127/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.2710 - accuracy: 0.8987\n",
      "Epoch 128/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 0.2737 - accuracy: 0.8988\n",
      "Epoch 129/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.2723 - accuracy: 0.8980\n",
      "Epoch 130/200\n",
      "52/52 [==============================] - 20s 394ms/step - loss: 0.2515 - accuracy: 0.9036\n",
      "Epoch 131/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.2529 - accuracy: 0.9050\n",
      "Epoch 132/200\n",
      "52/52 [==============================] - 20s 391ms/step - loss: 0.3205 - accuracy: 0.8858\n",
      "Epoch 133/200\n",
      "52/52 [==============================] - 21s 396ms/step - loss: 0.2787 - accuracy: 0.8936\n",
      "Epoch 134/200\n",
      "52/52 [==============================] - 20s 386ms/step - loss: 0.2993 - accuracy: 0.8870\n",
      "Epoch 135/200\n",
      "52/52 [==============================] - 20s 384ms/step - loss: 0.2650 - accuracy: 0.9026\n",
      "Epoch 136/200\n",
      "52/52 [==============================] - 19s 373ms/step - loss: 0.2603 - accuracy: 0.8997\n",
      "Epoch 137/200\n",
      "52/52 [==============================] - 20s 376ms/step - loss: 0.2741 - accuracy: 0.9007\n",
      "Epoch 138/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.2941 - accuracy: 0.8937\n",
      "Epoch 139/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.2727 - accuracy: 0.9003\n",
      "Epoch 140/200\n",
      "52/52 [==============================] - 20s 384ms/step - loss: 0.2521 - accuracy: 0.9045\n",
      "Epoch 141/200\n",
      "52/52 [==============================] - 20s 389ms/step - loss: 0.2462 - accuracy: 0.9074\n",
      "Epoch 142/200\n",
      "52/52 [==============================] - 20s 384ms/step - loss: 0.2452 - accuracy: 0.9111\n",
      "Epoch 143/200\n",
      "52/52 [==============================] - 20s 393ms/step - loss: 0.2511 - accuracy: 0.8967\n",
      "Epoch 144/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.2440 - accuracy: 0.9027\n",
      "Epoch 145/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.2403 - accuracy: 0.9002\n",
      "Epoch 146/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.2487 - accuracy: 0.9008\n",
      "Epoch 147/200\n",
      "52/52 [==============================] - 20s 389ms/step - loss: 0.2458 - accuracy: 0.9050\n",
      "Epoch 148/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.2389 - accuracy: 0.9118\n",
      "Epoch 149/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.2370 - accuracy: 0.9071\n",
      "Epoch 150/200\n",
      "52/52 [==============================] - 20s 389ms/step - loss: 0.2266 - accuracy: 0.9144\n",
      "Epoch 151/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.2285 - accuracy: 0.9052\n",
      "Epoch 152/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.2128 - accuracy: 0.9133\n",
      "Epoch 153/200\n",
      "52/52 [==============================] - 20s 386ms/step - loss: 0.2299 - accuracy: 0.9076\n",
      "Epoch 154/200\n",
      "52/52 [==============================] - 20s 382ms/step - loss: 0.2292 - accuracy: 0.9159\n",
      "Epoch 155/200\n",
      "52/52 [==============================] - 20s 379ms/step - loss: 0.2298 - accuracy: 0.9078\n",
      "Epoch 156/200\n",
      "52/52 [==============================] - 20s 384ms/step - loss: 0.2489 - accuracy: 0.9006\n",
      "Epoch 157/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.2245 - accuracy: 0.9145\n",
      "Epoch 158/200\n",
      "52/52 [==============================] - 20s 391ms/step - loss: 0.2544 - accuracy: 0.9072\n",
      "Epoch 159/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 0.2323 - accuracy: 0.9117\n",
      "Epoch 160/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 0.2397 - accuracy: 0.9043\n",
      "Epoch 161/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 0.2454 - accuracy: 0.9060\n",
      "Epoch 162/200\n",
      "52/52 [==============================] - 20s 391ms/step - loss: 0.2219 - accuracy: 0.9092\n",
      "Epoch 163/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.2510 - accuracy: 0.9037\n",
      "Epoch 164/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 0.2305 - accuracy: 0.9159\n",
      "Epoch 165/200\n",
      "52/52 [==============================] - 20s 386ms/step - loss: 0.2393 - accuracy: 0.9055\n",
      "Epoch 166/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 0.2377 - accuracy: 0.9027\n",
      "Epoch 167/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.2101 - accuracy: 0.9158\n",
      "Epoch 168/200\n",
      "52/52 [==============================] - 20s 383ms/step - loss: 0.2316 - accuracy: 0.9042\n",
      "Epoch 169/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 0.2116 - accuracy: 0.9138\n",
      "Epoch 170/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.2269 - accuracy: 0.9087\n",
      "Epoch 171/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.2381 - accuracy: 0.9083\n",
      "Epoch 172/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.2210 - accuracy: 0.9126\n",
      "Epoch 173/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.2290 - accuracy: 0.9125\n",
      "Epoch 174/200\n",
      "52/52 [==============================] - 20s 387ms/step - loss: 0.2494 - accuracy: 0.9003\n",
      "Epoch 175/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.1835 - accuracy: 0.9269\n",
      "Epoch 176/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 0.2319 - accuracy: 0.9045\n",
      "Epoch 177/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.2263 - accuracy: 0.9128\n",
      "Epoch 178/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.2540 - accuracy: 0.9040\n",
      "Epoch 179/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.2089 - accuracy: 0.9176\n",
      "Epoch 180/200\n",
      "52/52 [==============================] - 20s 386ms/step - loss: 0.2344 - accuracy: 0.8999\n",
      "Epoch 181/200\n",
      "52/52 [==============================] - 20s 378ms/step - loss: 0.2623 - accuracy: 0.9047\n",
      "Epoch 182/200\n",
      "52/52 [==============================] - 20s 385ms/step - loss: 0.2058 - accuracy: 0.9205\n",
      "Epoch 183/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.2122 - accuracy: 0.9219\n",
      "Epoch 184/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.2833 - accuracy: 0.8981\n",
      "Epoch 185/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.2317 - accuracy: 0.9123\n",
      "Epoch 186/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.2393 - accuracy: 0.9104\n",
      "Epoch 187/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.2202 - accuracy: 0.9115\n",
      "Epoch 188/200\n",
      "52/52 [==============================] - 20s 394ms/step - loss: 0.2137 - accuracy: 0.9170\n",
      "Epoch 189/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.2020 - accuracy: 0.9178\n",
      "Epoch 190/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.2002 - accuracy: 0.9212\n",
      "Epoch 191/200\n",
      "52/52 [==============================] - 20s 394ms/step - loss: 0.1922 - accuracy: 0.9260\n",
      "Epoch 192/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 0.2691 - accuracy: 0.9013\n",
      "Epoch 193/200\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.1738 - accuracy: 0.9268\n",
      "Epoch 194/200\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.2030 - accuracy: 0.9181\n",
      "Epoch 195/200\n",
      "52/52 [==============================] - 20s 392ms/step - loss: 0.2078 - accuracy: 0.9156\n",
      "Epoch 196/200\n",
      "52/52 [==============================] - 20s 386ms/step - loss: 0.2168 - accuracy: 0.9060\n",
      "Epoch 197/200\n",
      "52/52 [==============================] - 20s 382ms/step - loss: 0.2215 - accuracy: 0.9089\n",
      "Epoch 198/200\n",
      "52/52 [==============================] - 20s 386ms/step - loss: 0.1958 - accuracy: 0.9186\n",
      "Epoch 199/200\n",
      "52/52 [==============================] - 20s 378ms/step - loss: 0.2040 - accuracy: 0.9134\n",
      "Epoch 200/200\n",
      "52/52 [==============================] - 20s 384ms/step - loss: 0.2202 - accuracy: 0.9076\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.engine.sequential.Sequential at 0x7f23def2a450>,\n",
       " <keras.callbacks.History at 0x7f226824f590>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_network():\n",
    "    \"\"\" Train a Neural Network to generate music \"\"\"\n",
    "    \n",
    "    model = create_network(network_input, n_vocab)\n",
    "    print(model.summary())\n",
    " \n",
    "    checkpoint = ModelCheckpoint(\n",
    "        \"weights/weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\",\n",
    "        monitor='loss',\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        mode='min'\n",
    "    ) \n",
    "    \n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    # Your line of code here\n",
    "    print('Training...')\n",
    "    history = model.fit(network_input, network_output, batch_size=64, epochs=200, verbose=1, callbacks=callbacks_list)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "train_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply model & Generate music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from output_midi_utils import generate_from_random, generate_from_one_note, write_midi_from_generated_pianoroll, write_midi_from_generated_pretty_midi, generate_notes, play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: ('70', 13), 2: ('e', 2), 3: ('39,79', 5), 4: ('79', 6), 5: ('55,63,79', 5), 6: ('55,79', 1), 7: ('58,63,67,79', 6), 8: ('79', 1), 9: ('51,79', 3), 10: ('79', 3), 11: ('e', 1), 12: ('56,62,77', 7), 13: ('59,62,68,79', 8), 14: ('39,77', 4), 15: ('77', 5), 16: ('55,63,77', 6), 17: ('77', 1), 18: ('58,63,67,77', 7), 19: ('38,75', 3), 20: ('75', 4), 21: ('55,63,75', 6), 22: ('55', 1), 23: ('58,63,67,70', 9), 24: ('70', 1), 25: ('36,79', 3), 26: ('79', 4), 27: ('55,64,79', 5), 28: ('58,64,67,72', 1), 29: ('58,64,67', 2), 30: ('58,64,67,71', 1), 31: ('58,64,67', 1), 32: ('58,64,67,72', 3), 33: ('72', 2), 34: ('48,84', 3), 35: ('84', 5), 36: ('55,64,84', 6), 37: ('60,64,70,79', 8), 38: ('41,82', 4), 39: ('82', 5), 40: ('53,61,82', 6), 41: ('53,82', 1), 42: ('58,61,64,82', 7), 43: ('82', 1), 44: ('41,80', 3), 45: ('80', 4), 46: ('53,60,80', 7), 47: ('56,60,65,79', 8), 48: ('46,77', 4), 49: ('53,62,77', 6), 50: ('53,77', 1), 51: ('58,62,68,77', 7), 52: ('47,79', 3), 53: ('79', 5), 54: ('55,65,79', 6), 55: ('62,65,67,74', 8), 56: ('48,75', 4), 57: ('75', 5), 58: ('55,75', 1), 59: ('60,63,67,75', 7), 60: ('45,72', 3), 61: ('72', 4), 62: ('54,63,72', 6), 63: ('54,72', 1), 64: ('60,63,66,72', 9), 65: ('46,70', 4), 66: ('70', 5), 67: ('53,63,86', 6), 68: ('53', 1), 69: ('58,63,68,84', 7), 70: ('84', 1), 71: ('34,82', 4), 72: ('80', 3), 73: ('53,62,79', 3), 74: ('53,62,80', 3), 75: ('53,62', 1), 76: ('58,68,72', 3), 77: ('58,68', 1), 78: ('58,68,74', 4), 79: ('74', 1), 80: ('39,75', 4), 81: ('58,63,67,75', 7), 82: ('51', 3), 83: ('e', 3), 84: ('55,63', 6), 85: ('58,63,67,70', 10), 86: ('55,63,79', 6), 87: ('58,63,67,79', 8), 88: ('51,77', 4), 89: ('79', 2), 90: ('56,62,77', 4), 91: ('56,62,76', 3), 92: ('56', 1), 93: ('59,62,68,77', 3), 94: ('59,62,68', 1), 95: ('59,62,68,79', 3), 96: ('58,63,67,75', 6), 97: ('75', 1), 98: ('50,75', 3), 99: ('77', 2), 100: ('55,63,75', 3), 101: ('55,63', 1), 102: ('55,63,74', 3), 103: ('58,63,67,75', 2), 104: ('58,63,67', 2), 105: ('58,63,67,77', 3), 106: ('48,79', 4), 107: ('71', 3), 108: ('55,64,72', 3), 109: ('55,64', 1), 110: ('55,64,73', 3), 111: ('58,64,67,72', 2), 112: ('58,64,67,77', 3), 113: ('48,76', 2), 114: ('48', 1), 115: ('55,64,79', 3), 116: ('55,64,85', 3), 117: ('60,64,70,84', 3), 118: ('60,64,70', 1), 119: ('60,64,70,79', 4), 120: ('82', 4), 121: ('53,61,82', 7), 122: ('46,77', 2), 123: ('46', 1), 124: ('46,79', 1), 125: ('53,62,79', 1), 126: ('53,62,77', 1), 127: ('53,62', 2), 128: ('53,79', 1), 129: ('58,62,68,77', 1), 130: ('58,62,68', 2), 131: ('58,62,68,76', 2), 132: ('58,62,68,77', 2), 133: ('47,79', 7), 134: ('e', 7), 135: ('55,65,79', 4), 136: ('62,65,67,74', 4), 137: ('55,63,75', 7), 138: ('72', 1), 139: ('53,86', 1), 140: ('58,63,68,84', 4), 141: ('58,63,68', 3), 142: ('34,82', 3), 143: ('34', 1), 144: ('80', 1), 145: ('53,62,80', 1), 146: ('53,62,80', 2), 147: ('58,68,74', 5), 148: ('39,75', 5), 149: ('51,75', 3), 150: ('75', 2), 151: ('55,63,74', 7), 152: ('58,63,67,75', 9), 153: ('77', 6), 154: ('58,62,65,77', 6), 155: ('46,79', 3), 156: ('53,62,79', 5), 157: ('58,62,65,77', 11), 158: ('45,77', 4), 159: ('53,60,77', 6), 160: ('60,65,77', 6), 161: ('53,60,72', 5), 162: ('53,72', 1), 163: ('60,65,72', 10), 164: ('44,75', 4), 165: ('44', 4), 166: ('51,60,75', 3), 167: ('51,60', 3), 168: ('56,60,63,75', 4), 169: ('56,60,63', 3), 170: ('32,75', 5), 171: ('51,59,74', 3), 172: ('51,59,75', 3), 173: ('56,59,63,77', 7), 174: ('56,59,63,75', 2), 175: ('51,70', 3), 176: ('70', 4), 177: ('55,63,70', 6), 178: ('40,82', 7), 179: ('52,61,82', 7), 180: ('58,61,67,82', 7), 181: ('40,81', 6), 182: ('81', 2), 183: ('52,60,81', 6), 184: ('58,60,67,79', 8), 185: ('41,69,77', 7), 186: ('69,77', 1), 187: ('53,63,69,77', 7), 188: ('60,63,69,77', 7), 189: ('43,70,74', 7), 190: ('70,74', 1), 191: ('55,62,70,74', 6), 192: ('58,62,70,74', 8), 193: ('36,67,75', 7), 194: ('67,75', 1), 195: ('55,63,67,75', 6), 196: ('60,63,67,75', 8), 197: ('41,69,74', 3), 198: ('e', 5), 199: ('53,63,69,72', 3), 200: ('e', 4), 201: ('60,63,65,69,74', 4), 202: ('58,62,65,70', 4), 203: ('e', 6), 204: ('57,63,66,71', 3), 205: ('56,64,71', 3), 206: ('55,64,70,72', 2), 207: ('53,60,63,65,69,72', 4), 208: ('58,65,68,74', 5), 209: ('39,67', 1), 210: ('39,70', 1), 211: ('39,75', 2), 212: ('39,79', 6), 213: ('58,63,67,69', 2), 214: ('58,63,67', 1), 215: ('58,63,67,70', 2), 216: ('51,71', 3), 217: ('70', 3), 218: ('56,62,73', 3), 219: ('56,62,74', 4), 220: ('58,62,68,79', 7), 221: ('58,63,67,75', 5), 222: ('50', 1), 223: ('55,64,72', 2), 224: ('60,64,70,79', 5), 225: ('58,62,68', 1), 226: ('58,62,68,79', 1), 227: ('58,62,68,76', 1), 228: ('77', 3), 229: ('47,79', 4), 230: ('62,65,67,74', 7), 231: ('58,63,68,85', 3), 232: ('58,63,68', 4), 233: ('34,84', 1), 234: ('34,83', 1), 235: ('53,62,74', 1), 236: ('53,62,71', 2), 237: ('58,68,70', 2), 238: ('58,68,74', 3), 239: ('58,68,79', 2), 240: ('58,68,77', 1), 241: ('75', 6), 242: ('75', 3), 243: ('55,63,74', 6), 244: ('53,62,79', 6), 245: ('58,62,65,77', 8), 246: ('60,65,77', 7), 247: ('72', 3), 248: ('53,60,72', 6), 249: ('44,75', 6), 250: ('44', 1), 251: ('51,60,75', 2), 252: ('51,60', 1), 253: ('56,60,63', 1), 254: ('56,60,63,75', 7), 255: ('51,59,75', 4), 256: ('56,59,63', 1), 257: ('56,59,63,75', 1), 258: ('36,67,75', 6), 259: ('60,63,67,75', 9), 260: ('41,69,74', 4), 261: ('53,60,63,65,69,72', 2), 262: ('58,65,68,74', 8), 263: ('67', 1), 264: ('39,79', 4), 265: ('58,63,67,69', 3), 266: ('58,63,67,70', 3), 267: ('56,62', 1), 268: ('55,64,79', 2), 269: ('55,64,85', 4), 270: ('60,64,70,84', 4), 271: ('60,64,70,79', 2), 272: ('41,82', 8), 273: ('41,80', 6), 274: ('53,60,80', 6), 275: ('56,60,65,79', 9), 276: ('46,77', 1), 277: ('47', 3), 278: ('55,65,79', 7), 279: ('48,75', 7), 280: ('45,72', 6), 281: ('46,86', 3), 282: ('86', 1), 283: ('53,63,86', 4), 284: ('53,63', 1), 285: ('53,63,85', 2), 286: ('58,63,68,84', 1), 287: ('58,63,68', 2), 288: ('58,63,68,83', 2), 289: ('34,82', 1), 290: ('34', 2), 291: ('34,81', 1), 292: ('53,62,69', 1), 293: ('53,62,70', 1), 294: ('53,62,71', 1), 295: ('58,68,73', 2), 296: ('58,68,77', 2), 297: ('58,68,75', 3), 298: ('39,75', 8), 299: ('55,63,75', 5), 300: ('58,63,67,75', 4), 301: ('51,75', 6), 302: ('58,63,67,75', 10), 303: ('56,59,63,75', 6), 304: ('59,63,68,75', 7), 305: ('51,77', 7), 306: ('59,63,68,77', 8), 307: ('58,63,67,79', 10), 308: ('56,75', 1), 309: ('51,75', 4), 310: ('77', 4), 311: ('56,59,63,75', 3), 312: ('56,59,63,77', 2), 313: ('56,77', 1), 314: ('59,63,68,75', 3), 315: ('59,63,68,77', 4), 316: ('39,79', 3), 317: ('55,58,63,79', 6), 318: ('58,63,67,75', 1), 319: ('58,63,67,77', 1), 320: ('58,63,67,74', 1), 321: ('58,63,67,75', 3), 322: ('51,87', 4), 323: ('87', 3), 324: ('55,63,86', 5), 325: ('58,63,67,84', 7), 326: ('51,82', 8), 327: ('53,62,82', 7), 328: ('58,68,81', 6), 329: ('81', 1), 330: ('51,80', 7), 331: ('53,62,72', 7), 332: ('58,68,74', 7), 333: ('55,63,77', 2), 334: ('55,63', 3), 335: ('55,63,75', 2), 336: ('58,67,74', 3), 337: ('58,67', 1), 338: ('58,67,75', 1), 339: ('58,67,75', 4), 340: ('45,91', 4), 341: ('45', 3), 342: ('53,63,89', 2), 343: ('53,63', 2), 344: ('53,63,87', 1), 345: ('60,63,65,86', 1), 346: ('60,63,65', 2), 347: ('60,63,65,84', 2), 348: ('60,63,65', 1), 349: ('46,83', 7), 350: ('53,63,82', 7), 351: ('58,63,68,81', 7), 352: ('34,81', 3), 353: ('34,80', 2), 354: ('58,62,68,79', 8), 355: ('39,75', 7), 356: ('32,75', 9), 357: ('51,56,59,75', 6), 358: ('59,63,68', 1), 359: ('59,63,68,77', 3), 360: ('59,63,68,75', 1), 361: ('56,59,63,77', 1), 362: ('51,56,59', 1), 363: ('51,56,59,75', 2), 364: ('51,56,59,77', 4), 365: ('39,79', 7), 366: ('58,63,67,79', 5), 367: ('51', 6), 368: ('58,63,67', 8), 369: ('32,75', 3), 370: ('32', 4), 371: ('51,56,59,68', 4), 372: ('51,56,59,70', 2), 373: ('51,56,59,68', 1), 374: ('56,59,63,68', 3), 375: ('56,59,63,67', 3), 376: ('51,56,59,68', 3), 377: ('51,56,59,71', 3), 378: ('56,59,63,80', 2), 379: ('51,56,59,87', 4), 380: ('51,56,59', 4), 381: ('51,56,59,89', 1), 382: ('39,91', 6), 383: ('91', 1), 384: ('55,63,87', 8), 385: ('87', 1), 386: ('58,63,67,87,99', 6), 387: ('87,99', 1), 388: ('45,87,99', 7), 389: ('53,63,86,98', 7), 390: ('60,63,65,84,96', 7), 391: ('46,83,95', 8), 392: ('53,58,63,82,94', 6), 393: ('58,63,68,81,93', 7), 394: ('47,80,92', 6), 395: ('55,65,79,91', 7), 396: ('62,65,67,74,86', 8), 397: ('48,75,87', 8), 398: ('55,63,87,99', 6), 399: ('60,67,87,99', 6), 400: ('45,87,99', 3), 401: ('87,99', 3), 402: ('53,60,63,77,89', 6), 403: ('53,60,63,65,84,96', 11), 404: ('34,83,95', 8), 405: ('46,83,95', 5), 406: ('53,83,95', 4), 407: ('83,95', 1), 408: ('56,83,95', 3), 409: ('62,83,95', 4), 410: ('58,83,95', 4), 411: ('65,83,95', 5), 412: ('68,83,95', 7), 413: ('74,82,94', 47), 414: ('95', 3), 415: ('94', 2), 416: ('96', 2), 417: ('93', 1), 418: ('95', 1), 419: ('94', 1), 420: ('96', 1), 421: ('93', 2), 422: ('95', 2), 423: ('96', 3), 424: ('94', 3), 425: ('98', 2), 426: ('92', 1), 427: ('91', 2), 428: ('89', 2), 429: ('86', 3), 430: ('84', 2), 431: ('82', 3), 432: ('72', 5), 433: ('74', 5), 434: ('39,67,75', 9), 435: ('58,70', 7), 436: ('46,79', 6), 437: ('51,67,75', 7), 438: ('46,79', 7), 439: ('39,67,75', 7), 440: ('51,67,75', 6), 441: ('46,58,79', 9), 442: ('58,79', 1), 443: ('58', 1), 444: ('51,58,67,75', 22), 445: ('63,70,79,87', 24)}\n"
     ]
    }
   ],
   "source": [
    "print(note_tokenizer.index_to_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23dd3e1e05f46ba9dfa39f734441f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='genrt', max=500.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = create_network(network_input, n_vocab)\n",
    "model.load_weights('weights/weights-improvement-190-0.2054-bigger.hdf5')\n",
    "\n",
    "# global parameter\n",
    "tempo = 50\n",
    "max_generate = 500\n",
    "dest_path = \"Generated_MIDI/pianoroll_output/Chopin_random_1.mid\"\n",
    "\n",
    "# unique_notes = note_tokenizer.unique_word\n",
    "seq_len = sequence_length\n",
    "\n",
    "# generate the initial sequence from one note chosen manually\n",
    "# generate = generate_from_one_note(note_tokenizer, seq_len, ('39,75', 2))\n",
    "\n",
    "# generate the initial sequence all randomly\n",
    "generate = generate_from_random(unique_notes, seq_len)\n",
    "\n",
    "# generate the next \"max_generate\" notes in index form\n",
    "generate = generate_notes(generate, model, n_vocab, max_generate, seq_len)\n",
    "\n",
    "# generate output midi file\n",
    "track = write_midi_from_generated_pianoroll(note_tokenizer, generate, dest_path, start_index=seq_len-1, const_tempo=tempo, max_generated=max_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id='midiPlayerDiv28796'></div>\n",
       "                <link rel=\"stylesheet\" href=\"//cuthbertLab.github.io/music21j/css/m21.css\"\n",
       "                    type=\"text/css\" />\n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {'music21': '//cuthbertLab.github.io/music21j/src/music21'}\n",
       "                });\n",
       "                require(['music21'], function() {\n",
       "                               mp = new music21.miditools.MidiPlayer();\n",
       "                               mp.addPlayer('#midiPlayerDiv28796');\n",
       "                               mp.base64Load('data:audio/midi;base64,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');\n",
       "                        });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from music21 import midi as midi21\n",
    "from music21 import stream\n",
    "\n",
    "mf = midi21.MidiFile()\n",
    "mf.open(dest_path)\n",
    "mf.read()\n",
    "mf.close()\n",
    "s = midi21.translate.midiFileToStream(mf)\n",
    "\n",
    "myStream = stream.Stream()\n",
    "myStream.append(s)\n",
    "play(myStream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
